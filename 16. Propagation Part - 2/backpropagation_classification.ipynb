{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efh0Vbd75p02"
   },
   "source": [
    "# Classification Example of BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qsjhE0IG5qy5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,1],[7,9,1],[6,10,0],[5,5,0]], columns=['cgpa', 'profile_score', 'placed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "Z9vKYUso5y3F",
    "outputId": "921e5023-3af8-4552-d541-13371dd8655d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>profile_score</th>\n",
       "      <th>placed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa  profile_score  placed\n",
       "0     8              8       1\n",
       "1     7              9       1\n",
       "2     6             10       0\n",
       "3     5              5       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BSAs5N7L5z9f"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    This Function Initialize the value of parametrs like weights & Parameters \n",
    "    Initializes the parameters (weights and biases) for a neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_dims -- List containing the dimensions of each layer in the network.\n",
    "                  Example: [input_size, hidden_layer1_size, ..., output_size]\n",
    "\n",
    "    Returns:\n",
    "    parameters -- Python dictionary containing the initialized parameters:\n",
    "                  parameters[\"W1\"] = weight matrix for layer 1\n",
    "                  parameters[\"b1\"] = bias vector for layer 1\n",
    "                  ...\n",
    "                  parameters[\"WL\"] = weight matrix for the last layer\n",
    "                  parameters[\"bL\"] = bias vector for the last layer\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)  # Set a seed for the random number generator for reproducibility\n",
    "    parameters = {}    # Initialize an empty dictionary to store the parameters\n",
    "    L = len(layer_dims)  # Number of layers in the network, including input and output layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # Initialize weight matrix W for layer l with small values (here, all ones multiplied by 0.1)\n",
    "        parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l-1])) * 0.1\n",
    "        \n",
    "        # Initialize bias vector b for layer l with zeros\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters  # Return the dictionary containing all the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "In classification Activation function in SIgmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VDwatKWP543L"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    # Compute the sigmoid activation function\n",
    "    # Sigmoid function is defined as 1 / (1 + exp(-Z))\n",
    "    # It maps the input 'Z' to a value between 0 and 1\n",
    "    \n",
    "    A = 1 / (1 + np.exp(-Z))  # Apply the sigmoid function to 'Z'\n",
    "    \n",
    "    return A  # Return the output 'A', which is the result of the sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q7Pqn3Le6W88"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    # Perform the linear part of a layer's forward propagation.\n",
    "    # Compute the linear transformation: Z = W.T * A_prev + b\n",
    "    \n",
    "    Z = np.dot(W.T, A_prev) + b  # Calculate the linear combination of inputs and weights\n",
    "    \n",
    "    A = sigmoid(Z)  # Apply the sigmoid activation function to the linear output Z\n",
    "    \n",
    "    return A  # Return the activated output 'A'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VkUO9wdf6gXj"
   },
   "outputs": [],
   "source": [
    "def L_layer_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for the entire neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- Input data, of shape (input size, number of examples)\n",
    "    parameters -- Python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                  Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                  bl -- bias vector of shape (layer_dims[l], 1)\n",
    "\n",
    "    Returns:\n",
    "    A -- The output of the final layer, which is the prediction of the network\n",
    "    A_prev -- The output of the second last layer (useful for certain cases like computing the gradient in backpropagation)\n",
    "    \"\"\"\n",
    "\n",
    "    A = X  # Initialize activation for the input layer (A0 = X)\n",
    "    L = len(parameters) // 2  # Number of layers in the neural network (dividing by 2 since each layer has W and b)\n",
    "\n",
    "    # Loop over each layer to perform forward propagation\n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A  # Store the activation from the previous layer\n",
    "        \n",
    "        # Retrieve the parameters for the current layer l\n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        \n",
    "        # Perform the linear step for the current layer\n",
    "        A = linear_forward(A_prev, Wl, bl)\n",
    "        \n",
    "        # Uncomment these lines if you want to debug and see the values at each layer\n",
    "        #print(\"A\"+str(l-1)+\": \", A_prev)\n",
    "        #print(\"W\"+str(l)+\": \", Wl)\n",
    "        #print(\"b\"+str(l)+\": \", bl)\n",
    "        #print(\"--\" * 20)\n",
    "        #print(\"A\"+str(l)+\": \", A)\n",
    "        #print(\"**\" * 20)\n",
    "\n",
    "    return A, A_prev  # Return the output of the final layer and the second last layer's activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NKBtskel6jsf"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters,y,y_hat,A1,X):\n",
    "  parameters['W2'][0][0] = parameters['W2'][0][0] + (0.0001 * (y - y_hat)*A1[0][0])\n",
    "  parameters['W2'][1][0] = parameters['W2'][1][0] + (0.0001 * (y - y_hat)*A1[1][0])\n",
    "  parameters['b2'][0][0] = parameters['W2'][1][0] + (0.0001 * (y - y_hat))\n",
    "\n",
    "  parameters['W1'][0][0] = parameters['W1'][0][0] + (0.0001 * (y - y_hat)*parameters['W2'][0][0]*A1[0][0]*(1-A1[0][0])*X[0][0])\n",
    "  parameters['W1'][0][1] = parameters['W1'][0][1] + (0.0001 * (y - y_hat)*parameters['W2'][0][0]*A1[0][0]*(1-A1[0][0])*X[1][0])\n",
    "  parameters['b1'][0][0] = parameters['b1'][0][0] + (0.0001 * (y - y_hat)*parameters['W2'][0][0]*A1[0][0]*(1-A1[0][0]))\n",
    "\n",
    "  parameters['W1'][1][0] = parameters['W1'][1][0] + (0.0001 * (y - y_hat)*parameters['W2'][1][0]*A1[1][0]*(1-A1[1][0])*X[0][0])\n",
    "  parameters['W1'][1][1] = parameters['W1'][1][1] + (0.0001 * (y - y_hat)*parameters['W2'][1][0]*A1[1][0]*(1-A1[1][0])*X[1][0])\n",
    "  parameters['b1'][1][0] = parameters['b1'][1][0] + (0.0001 * (y - y_hat)*parameters['W2'][1][0]*A1[1][0]*(1-A1[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 1st Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRuFdbwf7uDz",
    "outputId": "ca7ef719-95c0-4e5f-fcf4-6bf3d30f76c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for this student -  0.613402628898913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10000513, 0.10000513],\n",
       "        [0.10000513, 0.10000513]]),\n",
       " 'b1': array([[6.41054186e-07],\n",
       "        [6.41054186e-07]]),\n",
       " 'W2': array([[0.10003815, 0.1       ],\n",
       "        [0.10003815, 0.1       ]]),\n",
       " 'b2': array([[0.100084]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the features 'cgpa' and 'profile_score' from the first row of the DataFrame\n",
    "# and reshape it into a column vector of shape (2, 1), where 2 is the number of features\n",
    "# and 1 is the number of training examples.\n",
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1)\n",
    "\n",
    "# Extract the target label 'placed' for the first row from the DataFrame.\n",
    "y = df[['placed']].values[0][0]\n",
    "\n",
    "# Initialize the parameters for a neural network with 2 input features, 2 neurons in the hidden layer, and 1 output neuron.\n",
    "parameters = initialize_parameters([2, 2, 1])\n",
    "\n",
    "# Perform forward propagation to compute the predicted value (y_hat) and the activation from the first layer (A1).\n",
    "y_hat, A1 = L_layer_forward(X, parameters)\n",
    "\n",
    "# Extract the scalar value from the predicted output (y_hat) for easier use in calculations.\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "# Update the parameters of the neural network based on the prediction error, using backpropagation.\n",
    "update_parameters(parameters, y, y_hat, A1, X)\n",
    "\n",
    "# Compute and print the loss (binary cross-entropy loss) for this specific training example.\n",
    "print('Loss for this student - ', -y*np.log(y_hat) - (1-y)*np.log(1-y_hat))\n",
    "\n",
    "# Display the updated parameters after the backpropagation step.\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 2nd Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHTFMij_8zJc",
    "outputId": "95e49589-f2a7-4451-98f2-04a5d2aa753a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for this student -  0.568725622654268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10000937, 0.10001059],\n",
       "        [0.10000937, 0.10001059]]),\n",
       " 'b1': array([[1.24770113e-06],\n",
       "        [1.24770113e-06]]),\n",
       " 'W2': array([[0.10007424, 0.1       ],\n",
       "        [0.10007424, 0.1       ]]),\n",
       " 'b2': array([[0.10011761]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the features 'cgpa' and 'profile_score' from the second row of the DataFrame\n",
    "# and reshape it into a column vector of shape (2, 1), where 2 is the number of features\n",
    "# and 1 is the number of training examples.\n",
    "X = df[['cgpa', 'profile_score']].values[1].reshape(2,1)\n",
    "\n",
    "# Extract the target label 'placed' for the second row from the DataFrame.\n",
    "y = df[['placed']].values[1][0]\n",
    "\n",
    "# Perform forward propagation to compute the predicted value (y_hat) and the activation from the first layer (A1).\n",
    "y_hat, A1 = L_layer_forward(X, parameters)\n",
    "\n",
    "# Extract the scalar value from the predicted output (y_hat) for easier use in calculations.\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "# Update the parameters of the neural network based on the prediction error, using backpropagation.\n",
    "update_parameters(parameters, y, y_hat, A1, X)\n",
    "\n",
    "# Compute and print the loss (binary cross-entropy loss) for this specific training example.\n",
    "print('Loss for this student - ', -y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "# Display the updated parameters after the backpropagation step.\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 3rd Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZCtTGoI90Jb",
    "outputId": "57944af6-ce79-4ad7-eea0-7576e788badb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for this student -  0.8353333695154365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10000463, 0.10000267],\n",
       "        [0.10000463, 0.10000267]]),\n",
       " 'b1': array([[4.56125378e-07],\n",
       "        [4.56135584e-07]]),\n",
       " 'W2': array([[0.10002712, 0.1       ],\n",
       "        [0.10002712, 0.1       ]]),\n",
       " 'b2': array([[0.09997049]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the features 'cgpa' and 'profile_score' from the third row of the DataFrame\n",
    "# and reshape it into a column vector of shape (2, 1), where 2 is the number of features\n",
    "# and 1 is the number of training examples.\n",
    "X = df[['cgpa', 'profile_score']].values[2].reshape(2,1)\n",
    "\n",
    "# Extract the target label 'placed' for the third row from the DataFrame.\n",
    "y = df[['placed']].values[2][0]\n",
    "\n",
    "# Perform forward propagation to compute the predicted value (y_hat) and the activation from the first layer (A1).\n",
    "y_hat, A1 = L_layer_forward(X, parameters)\n",
    "\n",
    "# Extract the scalar value from the predicted output (y_hat) for easier use in calculations.\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "# Update the parameters of the neural network based on the prediction error, using backpropagation.\n",
    "update_parameters(parameters, y, y_hat, A1, X)\n",
    "\n",
    "# Compute and print the loss (binary cross-entropy loss) for this specific training example.\n",
    "print('Loss for this student - ', -y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "# Display the updated parameters after the backpropagation step.\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 4th Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvIOVb8j94zz",
    "outputId": "6bda9dd3-38bd-4021-cd03-9f02c526e8a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for this student -  0.8239406691217557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10000386, 0.10000507],\n",
       "        [0.10000386, 0.10000507]]),\n",
       " 'b1': array([[1.43799372e-07],\n",
       "        [1.43805564e-07]]),\n",
       " 'W2': array([[0.1000332, 0.1      ],\n",
       "        [0.1000332, 0.1      ]]),\n",
       " 'b2': array([[0.09997707]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[3].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['placed']].values[3][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "print('Loss for this student - ',-y*np.log(y_hat) - (1-y)*np.log(1-y_hat))\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RTy2Hsy9-M7",
    "outputId": "774328a3-5614-4a66-8dac-425a271319b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  0.7103199085929446\n",
      "Epoch -  2 Loss -  0.6991702892802629\n",
      "Epoch -  3 Loss -  0.6991679314811485\n",
      "Epoch -  4 Loss -  0.6991655746710999\n",
      "Epoch -  5 Loss -  0.6991632188496667\n",
      "Epoch -  6 Loss -  0.699160864016399\n",
      "Epoch -  7 Loss -  0.6991585101708473\n",
      "Epoch -  8 Loss -  0.6991561573125619\n",
      "Epoch -  9 Loss -  0.6991538054410936\n",
      "Epoch -  10 Loss -  0.6991514545559935\n",
      "Epoch -  11 Loss -  0.6991491046568126\n",
      "Epoch -  12 Loss -  0.6991467557431024\n",
      "Epoch -  13 Loss -  0.6991444078144144\n",
      "Epoch -  14 Loss -  0.6991420608703007\n",
      "Epoch -  15 Loss -  0.6991397149103132\n",
      "Epoch -  16 Loss -  0.6991373699340042\n",
      "Epoch -  17 Loss -  0.6991350259409265\n",
      "Epoch -  18 Loss -  0.6991326829306324\n",
      "Epoch -  19 Loss -  0.6991303409026751\n",
      "Epoch -  20 Loss -  0.699127999856608\n",
      "Epoch -  21 Loss -  0.6991256597919842\n",
      "Epoch -  22 Loss -  0.6991233207083575\n",
      "Epoch -  23 Loss -  0.6991209826052818\n",
      "Epoch -  24 Loss -  0.699118645482311\n",
      "Epoch -  25 Loss -  0.6991163093389996\n",
      "Epoch -  26 Loss -  0.699113974174902\n",
      "Epoch -  27 Loss -  0.6991116399895729\n",
      "Epoch -  28 Loss -  0.6991093067825676\n",
      "Epoch -  29 Loss -  0.699106974553441\n",
      "Epoch -  30 Loss -  0.6991046433017485\n",
      "Epoch -  31 Loss -  0.6991023130270458\n",
      "Epoch -  32 Loss -  0.699099983728889\n",
      "Epoch -  33 Loss -  0.6990976554068338\n",
      "Epoch -  34 Loss -  0.6990953280604367\n",
      "Epoch -  35 Loss -  0.6990930016892543\n",
      "Epoch -  36 Loss -  0.6990906762928432\n",
      "Epoch -  37 Loss -  0.6990883518707602\n",
      "Epoch -  38 Loss -  0.6990860284225631\n",
      "Epoch -  39 Loss -  0.6990837059478086\n",
      "Epoch -  40 Loss -  0.6990813844460546\n",
      "Epoch -  41 Loss -  0.699079063916859\n",
      "Epoch -  42 Loss -  0.6990767443597797\n",
      "Epoch -  43 Loss -  0.6990744257743753\n",
      "Epoch -  44 Loss -  0.699072108160204\n",
      "Epoch -  45 Loss -  0.6990697915168249\n",
      "Epoch -  46 Loss -  0.6990674758437966\n",
      "Epoch -  47 Loss -  0.6990651611406782\n",
      "Epoch -  48 Loss -  0.6990628474070294\n",
      "Epoch -  49 Loss -  0.6990605346424095\n",
      "Epoch -  50 Loss -  0.6990582228463785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.09994267, 0.09984548],\n",
       "        [0.09994272, 0.09984548]]),\n",
       " 'b1': array([[-3.38405750e-05],\n",
       "        [-3.38419977e-05]]),\n",
       " 'W2': array([[0.09920806, 0.1       ],\n",
       "        [0.09920816, 0.1       ]]),\n",
       " 'b2': array([[0.09915209]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters([2,2,1])\n",
    "epochs = 50\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(df.shape[0]):\n",
    "\n",
    "    X = df[['cgpa', 'profile_score']].values[j].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "    y = df[['placed']].values[j][0]\n",
    "\n",
    "    # Parameter initialization\n",
    "\n",
    "\n",
    "    y_hat,A1 = L_layer_forward(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "    Loss.append(-y*np.log(y_hat) - (1-y)*np.log(1-y_hat))\n",
    "\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkBltTUj-M_g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
